
## 事故级别

`严重`

## 事故描述

* 19:30 QA收到玩家反馈，部分区服玩家登录游戏后找不到角色。QA通知运维。
* 19:35 运维响应，排查到是由于MongoDB数据库迁移，1组5个数据库数据未迁移，但CMDB数据已更新。游戏服务器找不到数据库，自动创建新库。
* 19:40 确认影响范围为5个区服，更新CMDB，让受影响区服连接未迁移之前的数据库。通知大神圈运维备份受影响区服所属的MongoDB。
* 19:50 重启受影响的区服，通知QA白名单验证。
* 20:10 受影响区服恢复，解维护。
* 20:28 MongoDB备份开始，导致2台负载较高的MongoDB和GameServer通信短暂失联，GameServer主动进入维护状态。运维收到告警。
* 20:32 运维确认服务正常，解维护进入自动维护的区服。游戏服务恢复正常。

## 事故影响

* 5个区服玩家丢失角色40分钟.
* 若干区服进入维护状态4分钟，部分玩家登陆受限，已在线玩家不受影响。

## 事故分析

* MongoDB迁移过程是运维手动操作，统计迁移列表时，错误的复制了一台Mongo的IP地址。
* 大神圈运维操作数据迁移时，发现该Mongo地址不对，数据并未迁移。
* 收到大神圈的通知后，由于维护时间较长，决定改Mongo迁移顺延到下周。但是忘记把CMDB中相应的数据恢复。
* 部分机器上负载了几十个MongoDB数据库，备份时短暂的连接超时会导致很大面积区服自动进入维护状态。

## 事故总结

* CMDB上的数据操作，有很多还依赖运维手工修改。在操作的数据量大时，操作没有审计，很容易出现纰漏。
* MongoDB迁移操作耗时长，导致区服维护时间长。
* 对大神圈的反馈没有足够的重视，没有想到要恢复CMDB的数据。

## 后续跟进

* 收集CMDB操作中需要大量手动参与的用例，优化工具，提高自动化水平，避免手工操作。
* 保持对不正常状况，来自QA和合作方提供的信息敏感，避免对收到的警告视而不见。
